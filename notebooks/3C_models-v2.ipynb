{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\influence\\sample_submission.csv\n",
      "data\\influence\\solution.csv\n",
      "data\\influence\\test.csv\n",
      "data\\influence\\train.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "DATA_DIR=Path('../data/influence')\n",
    "for dirname, _, filenames in os.walk(DATA_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "TEST_PATH = DATA_DIR / \"test.csv\"\n",
    "SOLUTION_PATH = DATA_DIR / \"solution.csv\"\n",
    "\n",
    "SUBMISSION_PATH = Path(\"../submissions/v2\")\n",
    "\n",
    "\n",
    "PURPOSE_LABELS = {\n",
    "    0: \"BACKGROUND\",\n",
    "    1: \"COMPARES_CONTRASTS\",\n",
    "    2: \"EXTENSION\",\n",
    "    3: \"FUTURE\",\n",
    "    4: \"MOTIVATION\",\n",
    "    5: \"USES\"\n",
    "}\n",
    "\n",
    "INFLUENCE_LABELS = {\n",
    "    0: \"INCIDENTAL\",\n",
    "    1: \"INFLUENTIAL\"\n",
    "}\n",
    "\n",
    "TASKS={\n",
    "    \"purpose\": [\"citation_class_label\", PURPOSE_LABELS],\n",
    "    \"influence\": [\"citation_influence_label\", INFLUENCE_LABELS]\n",
    "}\n",
    "\n",
    "np.random.seed(250320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'core_id', 'citing_title', 'citing_author', 'cited_title',\n",
       "       'cited_author', 'citation_context', 'citation_influence_label',\n",
       "       'citation_class_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH).merge(\n",
    "    pd.read_csv(str(TRAIN_PATH).replace(\"influence\", \"purpose\"))[[\"unique_id\", \"citation_class_label\"]],\n",
    "    on=\"unique_id\"\n",
    ")\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'core_id', 'citing_title', 'citing_author', 'cited_title',\n",
       "       'cited_author', 'citation_context'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(TEST_PATH).merge(\n",
    "    pd.read_csv(str(TEST_PATH).replace(\"influence\", \"purpose\"))[[\"unique_id\"]],\n",
    "    on=\"unique_id\"\n",
    ")\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'citation_influence_label', 'citation_class_label'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solution = pd.read_csv(SOLUTION_PATH).merge(\n",
    "    pd.read_csv(str(SOLUTION_PATH).replace(\"influence\", \"purpose\")),\n",
    "    on=\"unique_id\"\n",
    ")\n",
    "df_solution.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.merge(df_solution, on=\"unique_id\")\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>core_id</th>\n",
       "      <th>citing_title</th>\n",
       "      <th>citing_author</th>\n",
       "      <th>cited_title</th>\n",
       "      <th>cited_author</th>\n",
       "      <th>citation_context</th>\n",
       "      <th>citation_influence_label</th>\n",
       "      <th>citation_class_label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CC1</td>\n",
       "      <td>158977742</td>\n",
       "      <td>Ontology-Based Recommendation of Editorial Pro...</td>\n",
       "      <td>Thiviyan Thanapalasingam</td>\n",
       "      <td>Web search personalization with ontological us...</td>\n",
       "      <td>Sieg</td>\n",
       "      <td>They usually generate user models that describ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CC2</td>\n",
       "      <td>158977742</td>\n",
       "      <td>Ontology-Based Recommendation of Editorial Pro...</td>\n",
       "      <td>Thiviyan Thanapalasingam</td>\n",
       "      <td>Exploring Scholarly Data with Rexplore</td>\n",
       "      <td>Osborne</td>\n",
       "      <td>The Computer Science Ontology (CSO)[3]is a lar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CC3</td>\n",
       "      <td>158977742</td>\n",
       "      <td>Ontology-Based Recommendation of Editorial Pro...</td>\n",
       "      <td>Thiviyan Thanapalasingam</td>\n",
       "      <td>Klink-2: Integrating Multiple Web Sources to G...</td>\n",
       "      <td>Osborne</td>\n",
       "      <td>In order to do so, we characterized all SN pub...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CC4</td>\n",
       "      <td>158977742</td>\n",
       "      <td>Ontology-Based Recommendation of Editorial Pro...</td>\n",
       "      <td>Thiviyan Thanapalasingam</td>\n",
       "      <td>Forecasting the Spreading of Technologies in R...</td>\n",
       "      <td>Osborne</td>\n",
       "      <td>This API supports a number of applications, in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CC5</td>\n",
       "      <td>158977742</td>\n",
       "      <td>Ontology-Based Recommendation of Editorial Pro...</td>\n",
       "      <td>Thiviyan Thanapalasingam</td>\n",
       "      <td>Supporting Springer Nature Editors by means of...</td>\n",
       "      <td>Osborne</td>\n",
       "      <td>It works according to three main steps:1) It r...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id    core_id                                       citing_title  \\\n",
       "0       CC1  158977742  Ontology-Based Recommendation of Editorial Pro...   \n",
       "1       CC2  158977742  Ontology-Based Recommendation of Editorial Pro...   \n",
       "2       CC3  158977742  Ontology-Based Recommendation of Editorial Pro...   \n",
       "3       CC4  158977742  Ontology-Based Recommendation of Editorial Pro...   \n",
       "4       CC5  158977742  Ontology-Based Recommendation of Editorial Pro...   \n",
       "\n",
       "              citing_author  \\\n",
       "0  Thiviyan Thanapalasingam   \n",
       "1  Thiviyan Thanapalasingam   \n",
       "2  Thiviyan Thanapalasingam   \n",
       "3  Thiviyan Thanapalasingam   \n",
       "4  Thiviyan Thanapalasingam   \n",
       "\n",
       "                                         cited_title cited_author  \\\n",
       "0  Web search personalization with ontological us...         Sieg   \n",
       "1             Exploring Scholarly Data with Rexplore      Osborne   \n",
       "2  Klink-2: Integrating Multiple Web Sources to G...      Osborne   \n",
       "3  Forecasting the Spreading of Technologies in R...      Osborne   \n",
       "4  Supporting Springer Nature Editors by means of...      Osborne   \n",
       "\n",
       "                                    citation_context  \\\n",
       "0  They usually generate user models that describ...   \n",
       "1  The Computer Science Ontology (CSO)[3]is a lar...   \n",
       "2  In order to do so, we characterized all SN pub...   \n",
       "3  This API supports a number of applications, in...   \n",
       "4  It works according to three main steps:1) It r...   \n",
       "\n",
       "   citation_influence_label  citation_class_label  split  \n",
       "0                         0                     5  train  \n",
       "1                         0                     0  train  \n",
       "2                         0                     0  train  \n",
       "3                         1                     0  train  \n",
       "4                         1                     5  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    df_train.assign(split=\"train\"),\n",
    "    df_test.assign(split=\"test\"),\n",
    "], axis=0, sort=False).reset_index(drop=True).astype({task[0]: int for task in TASKS.values()})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    3000\n",
       "test     1000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citation_class_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>546</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>153</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split                 test  train\n",
       "citation_class_label             \n",
       "0                      546   1648\n",
       "5                      153    475\n",
       "1                      121    368\n",
       "4                      106    276\n",
       "2                       59    171\n",
       "3                       15     62"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(\n",
    "    index=\"citation_class_label\", columns=\"split\", values=\"unique_id\", aggfunc=len\n",
    ").sort_values(\"train\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 23045)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ColumnTransformer([\n",
    "    (\"citing_tfidf\", TfidfVectorizer(), \"citing_title\"),\n",
    "    (\"cited_tfidf\", TfidfVectorizer(), \"cited_title\"),\n",
    "    (\"citation_context_tfidf\", TfidfVectorizer(),\"citation_context\"),\n",
    "])\n",
    "ct.fit(df)\n",
    "df_features = ct.transform(df)\n",
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['submissions\\\\df_features.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save transformer\n",
    "dump(ct, SUBMISSION_PATH / \"ColumnTransformer.joblib\")\n",
    "dump(df_features, SUBMISSION_PATH / \"df_features.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4000x23045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 204727 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x23045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 93 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[[0, 1, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(df, label_col, split=\"train\"):\n",
    "    split_idx = df[(df.split == split)].index.tolist()\n",
    "    X = df_features[split_idx]\n",
    "    y = df.iloc[split_idx][label_col]\n",
    "    print(f\"{split}: X={X.shape}, y={y.shape}\")\n",
    "    return X, y, split_idx\n",
    "\n",
    "def submission_pipeline(model, df, df_features, task, model_key=None, to_dense=False):\n",
    "    # Setup submission folder\n",
    "    submission_folder = SUBMISSION_PATH / f\"{model_key}_{task}\"\n",
    "    submission_folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Generated folder: {submission_folder}\")\n",
    "    \n",
    "    model_file = submission_folder / \"model.joblib\"\n",
    "    submission_file=submission_folder / f\"submission.csv\"\n",
    "    \n",
    "    label_col, label_dict = TASKS[task]\n",
    "    \n",
    "    X_train, y_train, train_idx = generate_data(df, label_col, split=\"train\")\n",
    "    X_test, y_test, test_idx = generate_data(df, label_col, split=\"test\")\n",
    "    print(f\"Training model\")\n",
    "    if to_dense:\n",
    "        X_train = X_train.toarray()\n",
    "        X_test = X_test.toarray()\n",
    "    model.fit(X_train, y_train.astype(int))\n",
    "    dump(model, model_file)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print(\"Output label dist\")\n",
    "    print(pd.Series(y_test_pred).map(label_dict).value_counts())\n",
    "    \n",
    "    target_names = list(sorted(label_dict.values()))\n",
    "    \n",
    "    # Print reports \n",
    "    print(\"Training report\")\n",
    "    print(classification_report(y_train, y_train_pred, target_names=target_names))\n",
    "    print(\"Test report\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
    "    \n",
    "    train_report = classification_report(y_train, y_train_pred, target_names=target_names, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_test_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    print(f\"Writing submission file: {submission_file}\")\n",
    "    df.iloc[test_idx][[\"unique_id\"]].assign(**{label_col: y_test_pred}).to_csv(submission_file, index=False)\n",
    "    return model, train_report, test_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gbt\": [GradientBoostingClassifier, dict()],\n",
    "    \"rf\": [RandomForestClassifier, dict(n_jobs=-1)],\n",
    "    \"mlp-3\": [MLPClassifier, dict(hidden_layer_sizes=(256,256,128))],\n",
    "    \"mlp\": [MLPClassifier, dict()],\n",
    "    \"lr\": [LogisticRegressionCV, dict(n_jobs=-1)]\n",
    "}\n",
    "\n",
    "DENSE_MODELS = {\"mlp\", \"mlp-3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbt [<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, {}]\n",
      "Generated folder: submissions\\gbt_purpose\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n",
      "Output label dist\n",
      "BACKGROUND            936\n",
      "USES                   23\n",
      "COMPARES_CONTRASTS     19\n",
      "MOTIVATION             11\n",
      "FUTURE                  8\n",
      "EXTENSION               3\n",
      "dtype: int64\n",
      "Training report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.73      1.00      0.84      1648\n",
      "COMPARES_CONTRASTS       1.00      0.45      0.62       368\n",
      "         EXTENSION       0.99      0.68      0.81       171\n",
      "            FUTURE       1.00      1.00      1.00        62\n",
      "        MOTIVATION       1.00      0.47      0.64       276\n",
      "              USES       0.97      0.56      0.71       475\n",
      "\n",
      "          accuracy                           0.79      3000\n",
      "         macro avg       0.95      0.69      0.77      3000\n",
      "      weighted avg       0.85      0.79      0.78      3000\n",
      "\n",
      "Test report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.55      0.95      0.70       546\n",
      "COMPARES_CONTRASTS       0.26      0.04      0.07       121\n",
      "         EXTENSION       0.00      0.00      0.00        59\n",
      "            FUTURE       0.00      0.00      0.00        15\n",
      "        MOTIVATION       0.00      0.00      0.00       106\n",
      "              USES       0.43      0.07      0.11       153\n",
      "\n",
      "          accuracy                           0.53      1000\n",
      "         macro avg       0.21      0.18      0.15      1000\n",
      "      weighted avg       0.40      0.53      0.41      1000\n",
      "\n",
      "Writing submission file: submissions\\gbt_purpose\\submission.csv\n",
      "Wall time: 1min 3s\n",
      "Generated folder: submissions\\gbt_influence\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n",
      "Output label dist\n",
      "INFLUENTIAL    539\n",
      "INCIDENTAL     461\n",
      "dtype: int64\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.81      0.82      0.82      1568\n",
      " INFLUENTIAL       0.80      0.78      0.79      1432\n",
      "\n",
      "    accuracy                           0.80      3000\n",
      "   macro avg       0.80      0.80      0.80      3000\n",
      "weighted avg       0.80      0.80      0.80      3000\n",
      "\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.50      0.50      0.50       457\n",
      " INFLUENTIAL       0.58      0.57      0.57       543\n",
      "\n",
      "    accuracy                           0.54      1000\n",
      "   macro avg       0.54      0.54      0.54      1000\n",
      "weighted avg       0.54      0.54      0.54      1000\n",
      "\n",
      "Writing submission file: submissions\\gbt_influence\\submission.csv\n",
      "Wall time: 10.4 s\n",
      "rf [<class 'sklearn.ensemble.forest.RandomForestClassifier'>, {'n_jobs': -1}]\n",
      "Generated folder: submissions\\rf_purpose\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output label dist\n",
      "BACKGROUND            962\n",
      "USES                   23\n",
      "COMPARES_CONTRASTS     13\n",
      "MOTIVATION              2\n",
      "dtype: int64\n",
      "Training report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.95      1.00      0.97      1648\n",
      "COMPARES_CONTRASTS       0.99      0.92      0.96       368\n",
      "         EXTENSION       0.98      0.94      0.96       171\n",
      "            FUTURE       1.00      0.87      0.93        62\n",
      "        MOTIVATION       0.99      0.94      0.96       276\n",
      "              USES       1.00      0.93      0.97       475\n",
      "\n",
      "          accuracy                           0.97      3000\n",
      "         macro avg       0.98      0.93      0.96      3000\n",
      "      weighted avg       0.97      0.97      0.97      3000\n",
      "\n",
      "Test report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.55      0.96      0.70       546\n",
      "COMPARES_CONTRASTS       0.38      0.04      0.07       121\n",
      "         EXTENSION       0.00      0.00      0.00        59\n",
      "            FUTURE       0.00      0.00      0.00        15\n",
      "        MOTIVATION       0.00      0.00      0.00       106\n",
      "              USES       0.17      0.03      0.05       153\n",
      "\n",
      "          accuracy                           0.54      1000\n",
      "         macro avg       0.18      0.17      0.14      1000\n",
      "      weighted avg       0.37      0.54      0.40      1000\n",
      "\n",
      "Writing submission file: submissions\\rf_purpose\\submission.csv\n",
      "Wall time: 504 ms\n",
      "Generated folder: submissions\\rf_influence\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output label dist\n",
      "INCIDENTAL     537\n",
      "INFLUENTIAL    463\n",
      "dtype: int64\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.98      0.99      0.99      1568\n",
      " INFLUENTIAL       0.99      0.98      0.98      1432\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.98      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.45      0.53      0.49       457\n",
      " INFLUENTIAL       0.54      0.46      0.50       543\n",
      "\n",
      "    accuracy                           0.49      1000\n",
      "   macro avg       0.50      0.50      0.49      1000\n",
      "weighted avg       0.50      0.49      0.49      1000\n",
      "\n",
      "Writing submission file: submissions\\rf_influence\\submission.csv\n",
      "Wall time: 555 ms\n",
      "mlp-3 [<class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>, {'hidden_layer_sizes': (256, 256, 128)}]\n",
      "Generated folder: submissions\\mlp-3_purpose\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n",
      "Output label dist\n",
      "BACKGROUND            818\n",
      "USES                   90\n",
      "COMPARES_CONTRASTS     62\n",
      "MOTIVATION             29\n",
      "EXTENSION               1\n",
      "dtype: int64\n",
      "Training report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       1.00      1.00      1.00      1648\n",
      "COMPARES_CONTRASTS       1.00      1.00      1.00       368\n",
      "         EXTENSION       1.00      1.00      1.00       171\n",
      "            FUTURE       1.00      1.00      1.00        62\n",
      "        MOTIVATION       1.00      1.00      1.00       276\n",
      "              USES       1.00      1.00      1.00       475\n",
      "\n",
      "          accuracy                           1.00      3000\n",
      "         macro avg       1.00      1.00      1.00      3000\n",
      "      weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "Test report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.55      0.83      0.66       546\n",
      "COMPARES_CONTRASTS       0.26      0.13      0.17       121\n",
      "         EXTENSION       0.00      0.00      0.00        59\n",
      "            FUTURE       0.00      0.00      0.00        15\n",
      "        MOTIVATION       0.14      0.04      0.06       106\n",
      "              USES       0.22      0.13      0.16       153\n",
      "\n",
      "          accuracy                           0.49      1000\n",
      "         macro avg       0.20      0.19      0.18      1000\n",
      "      weighted avg       0.38      0.49      0.41      1000\n",
      "\n",
      "Writing submission file: submissions\\mlp-3_purpose\\submission.csv\n",
      "Wall time: 1min 42s\n",
      "Generated folder: submissions\\mlp-3_influence\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output label dist\n",
      "INFLUENTIAL    560\n",
      "INCIDENTAL     440\n",
      "dtype: int64\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       1.00      1.00      1.00      1568\n",
      " INFLUENTIAL       1.00      1.00      1.00      1432\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.45      0.44      0.44       457\n",
      " INFLUENTIAL       0.54      0.56      0.55       543\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.50      0.50      0.50      1000\n",
      "weighted avg       0.50      0.50      0.50      1000\n",
      "\n",
      "Writing submission file: submissions\\mlp-3_influence\\submission.csv\n",
      "Wall time: 1min 25s\n",
      "mlp [<class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>, {}]\n",
      "Generated folder: submissions\\mlp_purpose\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n",
      "Output label dist\n",
      "BACKGROUND            792\n",
      "USES                  104\n",
      "COMPARES_CONTRASTS     72\n",
      "MOTIVATION             28\n",
      "EXTENSION               3\n",
      "FUTURE                  1\n",
      "dtype: int64\n",
      "Training report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       1.00      1.00      1.00      1648\n",
      "COMPARES_CONTRASTS       1.00      1.00      1.00       368\n",
      "         EXTENSION       1.00      1.00      1.00       171\n",
      "            FUTURE       1.00      1.00      1.00        62\n",
      "        MOTIVATION       1.00      1.00      1.00       276\n",
      "              USES       1.00      1.00      1.00       475\n",
      "\n",
      "          accuracy                           1.00      3000\n",
      "         macro avg       1.00      1.00      1.00      3000\n",
      "      weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "Test report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.55      0.79      0.65       546\n",
      "COMPARES_CONTRASTS       0.24      0.14      0.18       121\n",
      "         EXTENSION       0.67      0.03      0.06        59\n",
      "            FUTURE       0.00      0.00      0.00        15\n",
      "        MOTIVATION       0.14      0.04      0.06       106\n",
      "              USES       0.20      0.14      0.16       153\n",
      "\n",
      "          accuracy                           0.48      1000\n",
      "         macro avg       0.30      0.19      0.19      1000\n",
      "      weighted avg       0.41      0.48      0.41      1000\n",
      "\n",
      "Writing submission file: submissions\\mlp_purpose\\submission.csv\n",
      "Wall time: 1min 44s\n",
      "Generated folder: submissions\\mlp_influence\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n",
      "Output label dist\n",
      "INFLUENTIAL    506\n",
      "INCIDENTAL     494\n",
      "dtype: int64\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       1.00      1.00      1.00      1568\n",
      " INFLUENTIAL       1.00      1.00      1.00      1432\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.45      0.49      0.47       457\n",
      " INFLUENTIAL       0.54      0.50      0.52       543\n",
      "\n",
      "    accuracy                           0.49      1000\n",
      "   macro avg       0.49      0.49      0.49      1000\n",
      "weighted avg       0.50      0.50      0.50      1000\n",
      "\n",
      "Writing submission file: submissions\\mlp_influence\\submission.csv\n",
      "Wall time: 1min 38s\n",
      "lr [<class 'sklearn.linear_model.logistic.LogisticRegressionCV'>, {'n_jobs': -1}]\n",
      "Generated folder: submissions\\lr_purpose\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output label dist\n",
      "BACKGROUND    999\n",
      "USES            1\n",
      "dtype: int64\n",
      "Training report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.63      1.00      0.78      1648\n",
      "COMPARES_CONTRASTS       0.00      0.00      0.00       368\n",
      "         EXTENSION       0.00      0.00      0.00       171\n",
      "            FUTURE       0.00      0.00      0.00        62\n",
      "        MOTIVATION       0.00      0.00      0.00       276\n",
      "              USES       0.99      0.84      0.91       475\n",
      "\n",
      "          accuracy                           0.68      3000\n",
      "         macro avg       0.27      0.31      0.28      3000\n",
      "      weighted avg       0.51      0.68      0.57      3000\n",
      "\n",
      "Test report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        BACKGROUND       0.55      1.00      0.71       546\n",
      "COMPARES_CONTRASTS       0.00      0.00      0.00       121\n",
      "         EXTENSION       0.00      0.00      0.00        59\n",
      "            FUTURE       0.00      0.00      0.00        15\n",
      "        MOTIVATION       0.00      0.00      0.00       106\n",
      "              USES       1.00      0.01      0.01       153\n",
      "\n",
      "          accuracy                           0.55      1000\n",
      "         macro avg       0.26      0.17      0.12      1000\n",
      "      weighted avg       0.45      0.55      0.39      1000\n",
      "\n",
      "Writing submission file: submissions\\lr_purpose\\submission.csv\n",
      "Wall time: 8.29 s\n",
      "Generated folder: submissions\\lr_influence\n",
      "train: X=(3000, 23045), y=(3000,)\n",
      "test: X=(1000, 23045), y=(1000,)\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output label dist\n",
      "INCIDENTAL    1000\n",
      "dtype: int64\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.52      1.00      0.69      1568\n",
      " INFLUENTIAL       0.00      0.00      0.00      1432\n",
      "\n",
      "    accuracy                           0.52      3000\n",
      "   macro avg       0.26      0.50      0.34      3000\n",
      "weighted avg       0.27      0.52      0.36      3000\n",
      "\n",
      "Test report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  INCIDENTAL       0.46      1.00      0.63       457\n",
      " INFLUENTIAL       0.00      0.00      0.00       543\n",
      "\n",
      "    accuracy                           0.46      1000\n",
      "   macro avg       0.23      0.50      0.31      1000\n",
      "weighted avg       0.21      0.46      0.29      1000\n",
      "\n",
      "Writing submission file: submissions\\lr_influence\\submission.csv\n",
      "Wall time: 2.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "reports = {}\n",
    "for model_key, model_params in model_configs.items():\n",
    "    model_cls, model_kwargs = model_params\n",
    "    to_dense=False\n",
    "    if model_cls in DENSE_MODELS:\n",
    "        to_dense=True\n",
    "    print(model_key, model_params)\n",
    "    for task in TASKS:\n",
    "        model = model_cls(**model_kwargs)\n",
    "        model, train_report, test_report = %time submission_pipeline(model, df, df_features, task, model_key=model_key, to_dense=to_dense)\n",
    "        reports[(model_key, task)] = {\"train\": train_report, \"test\": test_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports = pd.concat([\n",
    "    pd.concat([\n",
    "        pd.DataFrame(report[split]).T.assign(model=model, task=task, split=split).reset_index().rename(columns={\"index\": \"label\"})\n",
    "        for split in report\n",
    "    ])\n",
    "    for (model, task), report in reports.items()\n",
    "], axis=0, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>0.730018</td>\n",
       "      <td>0.997573</td>\n",
       "      <td>0.843077</td>\n",
       "      <td>1648.000</td>\n",
       "      <td>gbt</td>\n",
       "      <td>purpose</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>COMPARES_CONTRASTS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.451087</td>\n",
       "      <td>0.621723</td>\n",
       "      <td>368.000</td>\n",
       "      <td>gbt</td>\n",
       "      <td>purpose</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>EXTENSION</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>0.678363</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>171.000</td>\n",
       "      <td>gbt</td>\n",
       "      <td>purpose</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>FUTURE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>gbt</td>\n",
       "      <td>purpose</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MOTIVATION</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.471014</td>\n",
       "      <td>0.640394</td>\n",
       "      <td>276.000</td>\n",
       "      <td>gbt</td>\n",
       "      <td>purpose</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>INCIDENTAL</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.627316</td>\n",
       "      <td>457.000</td>\n",
       "      <td>lr</td>\n",
       "      <td>influence</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>INFLUENTIAL</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>543.000</td>\n",
       "      <td>lr</td>\n",
       "      <td>influence</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>lr</td>\n",
       "      <td>influence</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.313658</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>lr</td>\n",
       "      <td>influence</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.286684</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>lr</td>\n",
       "      <td>influence</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  label  precision    recall  f1-score   support model  \\\n",
       "0            BACKGROUND   0.730018  0.997573  0.843077  1648.000   gbt   \n",
       "1    COMPARES_CONTRASTS   1.000000  0.451087  0.621723   368.000   gbt   \n",
       "2             EXTENSION   0.991453  0.678363  0.805556   171.000   gbt   \n",
       "3                FUTURE   1.000000  1.000000  1.000000    62.000   gbt   \n",
       "4            MOTIVATION   1.000000  0.471014  0.640394   276.000   gbt   \n",
       "..                  ...        ...       ...       ...       ...   ...   \n",
       "135          INCIDENTAL   0.457000  1.000000  0.627316   457.000    lr   \n",
       "136         INFLUENTIAL   0.000000  0.000000  0.000000   543.000    lr   \n",
       "137            accuracy   0.457000  0.457000  0.457000     0.457    lr   \n",
       "138           macro avg   0.228500  0.500000  0.313658  1000.000    lr   \n",
       "139        weighted avg   0.208849  0.457000  0.286684  1000.000    lr   \n",
       "\n",
       "          task  split  \n",
       "0      purpose  train  \n",
       "1      purpose  train  \n",
       "2      purpose  train  \n",
       "3      purpose  train  \n",
       "4      purpose  train  \n",
       "..         ...    ...  \n",
       "135  influence   test  \n",
       "136  influence   test  \n",
       "137  influence   test  \n",
       "138  influence   test  \n",
       "139  influence   test  \n",
       "\n",
       "[140 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th colspan=\"2\" halign=\"left\">influence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">purpose</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>gbt</td>\n",
       "      <td>0.536886</td>\n",
       "      <td>0.804022</td>\n",
       "      <td>0.147578</td>\n",
       "      <td>0.769884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lr</td>\n",
       "      <td>0.313658</td>\n",
       "      <td>0.343257</td>\n",
       "      <td>0.119964</td>\n",
       "      <td>0.280571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp</td>\n",
       "      <td>0.493785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185423</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp-3</td>\n",
       "      <td>0.495649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176915</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf</td>\n",
       "      <td>0.491982</td>\n",
       "      <td>0.985290</td>\n",
       "      <td>0.136282</td>\n",
       "      <td>0.957873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task  influence             purpose          \n",
       "split      test     train      test     train\n",
       "model                                        \n",
       "gbt    0.536886  0.804022  0.147578  0.769884\n",
       "lr     0.313658  0.343257  0.119964  0.280571\n",
       "mlp    0.493785  1.000000  0.185423  1.000000\n",
       "mlp-3  0.495649  1.000000  0.176915  1.000000\n",
       "rf     0.491982  0.985290  0.136282  0.957873"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reports.loc[\n",
    "    df_reports.label==\"macro avg\", \n",
    "    [\"f1-score\", \"model\", \"task\", \"split\"]\n",
    "].pivot_table(index=\"model\", columns=[\"task\", \"split\"], values=\"f1-score\", aggfunc=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "split &   test &  train \\\\\n",
      "model &        &        \\\\\n",
      "\\midrule\n",
      "lr    &  0.120 &  0.281 \\\\\n",
      "rf    &  0.136 &  0.958 \\\\\n",
      "gbt   &  0.148 &  0.770 \\\\\n",
      "mlp-3 &  0.177 &  1.000 \\\\\n",
      "mlp   &  0.185 &  1.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lr</td>\n",
       "      <td>0.119964</td>\n",
       "      <td>0.280571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf</td>\n",
       "      <td>0.136282</td>\n",
       "      <td>0.957873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gbt</td>\n",
       "      <td>0.147578</td>\n",
       "      <td>0.769884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp-3</td>\n",
       "      <td>0.176915</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp</td>\n",
       "      <td>0.185423</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split      test     train\n",
       "model                    \n",
       "lr     0.119964  0.280571\n",
       "rf     0.136282  0.957873\n",
       "gbt    0.147578  0.769884\n",
       "mlp-3  0.176915  1.000000\n",
       "mlp    0.185423  1.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t = df_reports.loc[\n",
    "    (df_reports.label==\"macro avg\") & (df_reports.task==\"purpose\"), \n",
    "    [\"f1-score\", \"model\", \"task\", \"split\"]\n",
    "].pivot_table(index=\"model\", columns=\"split\", values=\"f1-score\", aggfunc=\"first\").sort_values(\"test\")\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    print(df_t.to_latex())\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "split &   test &  train \\\\\n",
      "model &        &        \\\\\n",
      "\\midrule\n",
      "lr    &  0.314 &  0.343 \\\\\n",
      "rf    &  0.492 &  0.985 \\\\\n",
      "mlp   &  0.494 &  1.000 \\\\\n",
      "mlp-3 &  0.496 &  1.000 \\\\\n",
      "gbt   &  0.537 &  0.804 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lr</td>\n",
       "      <td>0.313658</td>\n",
       "      <td>0.343257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf</td>\n",
       "      <td>0.491982</td>\n",
       "      <td>0.985290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp</td>\n",
       "      <td>0.493785</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp-3</td>\n",
       "      <td>0.495649</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gbt</td>\n",
       "      <td>0.536886</td>\n",
       "      <td>0.804022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split      test     train\n",
       "model                    \n",
       "lr     0.313658  0.343257\n",
       "rf     0.491982  0.985290\n",
       "mlp    0.493785  1.000000\n",
       "mlp-3  0.495649  1.000000\n",
       "gbt    0.536886  0.804022"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t = df_reports.loc[\n",
    "    (df_reports.label==\"macro avg\") & (df_reports.task==\"influence\"), \n",
    "    [\"f1-score\", \"model\", \"task\", \"split\"]\n",
    "].pivot_table(index=\"model\", columns=\"split\", values=\"f1-score\", aggfunc=\"first\").sort_values(\"test\")\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    print(df_t.to_latex())\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrr}\n",
      "\\toprule\n",
      "label &  BACKGROUND &  COMPARES\\_CONTRASTS &  EXTENSION &  FUTURE &  MOTIVATION &   USES &  accuracy &  macro avg &  weighted avg \\\\\n",
      "model &             &                     &            &         &             &        &           &            &               \\\\\n",
      "\\midrule\n",
      "lr    &       0.707 &               0.000 &      0.000 &     0.0 &       0.000 &  0.013 &     0.547 &      0.120 &         0.388 \\\\\n",
      "rf    &       0.698 &               0.075 &      0.000 &     0.0 &       0.000 &  0.045 &     0.535 &      0.136 &         0.397 \\\\\n",
      "gbt   &       0.700 &               0.071 &      0.000 &     0.0 &       0.000 &  0.114 &     0.534 &      0.148 &         0.408 \\\\\n",
      "mlp-3 &       0.663 &               0.175 &      0.000 &     0.0 &       0.059 &  0.165 &     0.492 &      0.177 &         0.414 \\\\\n",
      "mlp   &       0.649 &               0.176 &      0.065 &     0.0 &       0.060 &  0.163 &     0.478 &      0.185 &         0.411 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>COMPARES_CONTRASTS</th>\n",
       "      <th>EXTENSION</th>\n",
       "      <th>FUTURE</th>\n",
       "      <th>MOTIVATION</th>\n",
       "      <th>USES</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lr</td>\n",
       "      <td>0.706796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.119964</td>\n",
       "      <td>0.387898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf</td>\n",
       "      <td>0.697613</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.136282</td>\n",
       "      <td>0.396881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gbt</td>\n",
       "      <td>0.700405</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.147578</td>\n",
       "      <td>0.408450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp-3</td>\n",
       "      <td>0.662757</td>\n",
       "      <td>0.174863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>0.164609</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.176915</td>\n",
       "      <td>0.414490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp</td>\n",
       "      <td>0.648729</td>\n",
       "      <td>0.176166</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.163424</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.185423</td>\n",
       "      <td>0.410661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label  BACKGROUND  COMPARES_CONTRASTS  EXTENSION  FUTURE  MOTIVATION  \\\n",
       "model                                                                  \n",
       "lr       0.706796            0.000000   0.000000     0.0    0.000000   \n",
       "rf       0.697613            0.074627   0.000000     0.0    0.000000   \n",
       "gbt      0.700405            0.071429   0.000000     0.0    0.000000   \n",
       "mlp-3    0.662757            0.174863   0.000000     0.0    0.059259   \n",
       "mlp      0.648729            0.176166   0.064516     0.0    0.059701   \n",
       "\n",
       "label      USES  accuracy  macro avg  weighted avg  \n",
       "model                                               \n",
       "lr     0.012987     0.547   0.119964      0.387898  \n",
       "rf     0.045455     0.535   0.136282      0.396881  \n",
       "gbt    0.113636     0.534   0.147578      0.408450  \n",
       "mlp-3  0.164609     0.492   0.176915      0.414490  \n",
       "mlp    0.163424     0.478   0.185423      0.410661  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t = df_reports.loc[\n",
    "    (df_reports.split==\"test\") & (df_reports.task==\"purpose\"), \n",
    "    [\"label\", \"f1-score\", \"model\",]\n",
    "].pivot_table(index=\"model\", columns=\"label\", values=\"f1-score\", aggfunc=\"first\").sort_values(\"macro avg\")\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    print(df_t.to_latex())\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "label &  INCIDENTAL &  INFLUENTIAL &  accuracy &  macro avg &  weighted avg \\\\\n",
      "model &             &              &           &            &               \\\\\n",
      "\\midrule\n",
      "lr    &       0.627 &        0.000 &     0.457 &      0.314 &         0.287 \\\\\n",
      "rf    &       0.489 &        0.495 &     0.492 &      0.492 &         0.492 \\\\\n",
      "mlp   &       0.469 &        0.519 &     0.495 &      0.494 &         0.496 \\\\\n",
      "mlp-3 &       0.444 &        0.548 &     0.501 &      0.496 &         0.500 \\\\\n",
      "gbt   &       0.499 &        0.575 &     0.540 &      0.537 &         0.540 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>INCIDENTAL</th>\n",
       "      <th>INFLUENTIAL</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lr</td>\n",
       "      <td>0.627316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.313658</td>\n",
       "      <td>0.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf</td>\n",
       "      <td>0.488934</td>\n",
       "      <td>0.495030</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.491982</td>\n",
       "      <td>0.492244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp</td>\n",
       "      <td>0.468980</td>\n",
       "      <td>0.518589</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.493785</td>\n",
       "      <td>0.495918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mlp-3</td>\n",
       "      <td>0.443701</td>\n",
       "      <td>0.547597</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.495649</td>\n",
       "      <td>0.500117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gbt</td>\n",
       "      <td>0.498911</td>\n",
       "      <td>0.574861</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.536886</td>\n",
       "      <td>0.540152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label  INCIDENTAL  INFLUENTIAL  accuracy  macro avg  weighted avg\n",
       "model                                                            \n",
       "lr       0.627316     0.000000     0.457   0.313658      0.286684\n",
       "rf       0.488934     0.495030     0.492   0.491982      0.492244\n",
       "mlp      0.468980     0.518589     0.495   0.493785      0.495918\n",
       "mlp-3    0.443701     0.547597     0.501   0.495649      0.500117\n",
       "gbt      0.498911     0.574861     0.540   0.536886      0.540152"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t = df_reports.loc[\n",
    "    (df_reports.split==\"test\") & (df_reports.task==\"influence\"), \n",
    "    [\"label\", \"f1-score\", \"model\",]\n",
    "].pivot_table(index=\"model\", columns=\"label\", values=\"f1-score\", aggfunc=\"first\").sort_values(\"macro avg\")\n",
    "with pd.option_context(\"precision\", 3):\n",
    "    print(df_t.to_latex())\n",
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}